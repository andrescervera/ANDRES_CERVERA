{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Term Frequency - Inverse Document Frequency\n",
        "\n",
        "Aunque en este punto ya tenemos una buena representación vectorial de nuestros textos, sigue habiendo un problema: **la representación creada no está normalizada**. Esta no normalización plantea dos **problemas**:\n",
        "\n",
        "- A nivel de *documento* (las filas de nuestra matriz de datos) **cada texto** lleva una escala completamente libre y hace **imposible compararlos** entre sí. Un **texto más largo tendrá contadores con valores mayores que un texto más corto**. En un ejemplo llevado al extremo podemos comparar un tuit con la noticia que enlaza ese tuit. Ambos documentos versarán sobre el mismo tema, pero no podrán compararse debido a la volumetría de ambos.\n",
        "\n",
        "- A nivel de *palabras* es complicado **comparar cuáles son más relevantes y cuáles menos en un _corpus_ concreto**. Ya hemos eliminado las *palabras de parada*, pero, dependiendo del **bias** de nuestro *corpus* hay **palabras que no aportan demasiada información** y, por tanto, su incidencia en nuestro algoritmo de *machine learning* debería ser menor. Por ejemplo, imaginemos que tenemos un *corpus* de documentos que sólo hablan de los equipos de la Liga de Fútbol Profesional. En este corpus la palabra *\"fútbol \"* es completamente irrelevante, ya que todos los documentos hablan de ella. Por el contrario, palabras como *\"lesión \"* o *\"fichaje \"* son muy relevantes porque permiten subclasificar los documentos. Sin embargo, si nuestro *corpus* está formado por noticias de todo tipo, la palabra *'fútbol'* es muy relevante porque identifica un tipo de noticia.\n",
        "\n",
        "Para resolver este problema, se utiliza una normalización llamada **tf-idf** (*term-frecuency times inverse document-frecuency*). Se define mediante la siguiente ecuación\n",
        "\n",
        "$\\textrm{tf-idf}(t, d) = tf(t, d) \\times idf(t)$\n",
        "\n",
        "siendo $tf(t, d)$ el número de veces que el término (palabra) $t$ aparece en el documento $d$ y $idf(t)$ se define como:\n",
        "\n",
        "$idf(t) = log \\frac{1 + n}{1 + df(t)} + 1$\n",
        "\n",
        "donde $n$ es el número de documentos de nuestro *corpus* y $df(t)$ es el número de documentos en los que aparece el término $t$.\n",
        "\n",
        "Posteriormente, los vectores se normalizan a nivel de documento (el módulo del vector de cada documento es 1).\n",
        "\n",
        "Analizando estas ecuaciones *tf-idf* observamos que aquellas palabras que tengan menor frecuencia de aparición serán más relevantes que las que aparezcan en más documentos.\n",
        "\n",
        "Esta transformación se puede realizar utilizando el objeto [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n",
        "\n",
        "Para ver cómo funciona, vamos a construir un *corpus* con varios documentos:"
      ],
      "metadata": {
        "id": "HMotsTkmEV3r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-02T14:14:26.420752Z",
          "start_time": "2020-03-02T14:14:26.416011Z"
        },
        "id": "V7_qyymMhBKn"
      },
      "source": [
        "corpus = [\n",
        "    \"Este es el primer documento...\",\n",
        "    \"Este documento no es el primero, sino el segundo documento.\",\n",
        "    \"Y éste es el tercer documento.\",\n",
        "    \"¿Es éste el primero? No, es el cuarto documento.\"\n",
        "]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminamos caracteres extraños:"
      ],
      "metadata": {
        "id": "BKebSWAYHfE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# añadimos algunos más que no están en string.punctuation, como las comillas y\n",
        "# las aperturas de interrogación/exclamación\n",
        "# si no los añadiésemos, no se eliminarían\n",
        "chars = string.punctuation + '“”¡¿'\n",
        "\n",
        "re_punc = re.compile('[%s]' % re.escape(chars))\n",
        "# eliminar la puntuación de cada palabra\n",
        "corpus = [re_punc.sub('', texto) for texto in corpus]\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "Oa9Hqi8FCx6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3bc5a5-da71-4b2f-acf9-38db15051402"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Este es el primer documento', 'Este documento no es el primero sino el segundo documento', 'Y éste es el tercer documento', 'Es éste el primero No es el cuarto documento']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y acentos:"
      ],
      "metadata": {
        "id": "R8XfSoMCCx6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FmlPUKfoCx6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f09ac66-9f0b-4450-a62c-fdb5640e909a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Este es el primer documento', 'Este documento no es el primero sino el segundo documento', 'Y este es el tercer documento', 'Es este el primero No es el cuarto documento']\n"
          ]
        }
      ],
      "source": [
        "letras_con_acentos = [\n",
        "    'á', 'é', 'í', 'ó', 'ú'\n",
        "]\n",
        "letras_sin_acentos = [\n",
        "    'a', 'e', 'i', 'o', 'u'\n",
        "]\n",
        "\n",
        "def quita_acentos(texto) -> str:\n",
        "    res = texto\n",
        "    for lca, lsa in zip(letras_con_acentos, letras_sin_acentos):\n",
        "        res = res.replace(lca, lsa)\n",
        "    return res\n",
        "\n",
        "corpus = [quita_acentos(texto) for texto in corpus]\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertimos el texto a minúsculas:"
      ],
      "metadata": {
        "id": "Insh8G8DCx6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [texto.lower() for texto in corpus]\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "1Gp5lOnvCx6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ffebd1-26d3-4351-b6ff-6373a18598ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este es el primer documento', 'este documento no es el primero sino el segundo documento', 'y este es el tercer documento', 'es este el primero no es el cuarto documento']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos la técnica Bag of Words indicándole las stop words:"
      ],
      "metadata": {
        "id": "ObsYLa7RDbd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('spanish')"
      ],
      "metadata": {
        "id": "gr4bPoOcCx6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ef8105-87b5-4cb1-9bff-ef02ccbf4166"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "X = count_vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "mx7bXMI0H8hS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bccdcd-b62e-41ab-b571-cf7de7160b08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 0 0 0 0]\n",
            " [0 2 0 1 1 1 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [1 1 0 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "uZt0XRG9Cx6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e3433b-59f8-40ca-c3ce-e95c64236cb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cuarto' 'documento' 'primer' 'primero' 'segundo' 'sino' 'tercer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasta aquí nada nuevo en el horizonte. Veamos ahora como aplicar la transformación TF-IDF:"
      ],
      "metadata": {
        "id": "APf2PrZmICey"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-02T14:14:41.456651Z",
          "start_time": "2020-03-02T14:14:41.451749Z"
        },
        "id": "K5gY-SmgguQY"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-02T14:14:47.085506Z",
          "start_time": "2020-03-02T14:14:47.075349Z"
        },
        "id": "_3ELNstaiUOO"
      },
      "source": [
        "counts = X.toarray()\n",
        "X_transformed = tfidf_transformer.fit_transform(counts)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnsUWNcpidic"
      },
      "source": [
        "Y analizamos el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-02T14:14:52.625002Z",
          "start_time": "2020-03-02T14:14:52.619476Z"
        },
        "id": "F5SJFW4Bie5C",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556031cc-2eee-45f9-ea51-df43e46078f2"
      },
      "source": [
        "print(X_transformed.toarray())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46263733 0.88654763 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.54178991 0.         0.40927504 0.51911349 0.51911349\n",
            "  0.        ]\n",
            " [0.         0.46263733 0.         0.         0.         0.\n",
            "  0.88654763]\n",
            " [0.72664149 0.37919167 0.         0.5728925  0.         0.\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fijaos que ahora las ocurrencias están ponderadas de acuerdo a las ecuaciones que hemos visto antes:\n",
        "\n",
        "$\\textrm{tf-idf}(t, d) = tf(t, d) \\times idf(t)$\n",
        "\n",
        "siendo $tf(t, d)$ el número de veces que el término (palabra) $t$ aparece en el documento $d$ y $idf(t)$:\n",
        "\n",
        "$idf(t) = log \\frac{1 + n}{1 + df(t)} + 1$\n",
        "\n",
        "donde $n$ es el número de documentos de nuestro *corpus* y $df(t)$ es el número de documentos en los que aparece el término $t$.\n",
        "\n",
        "De esta forma, los vectores están normalizados a nivel de documento (el módulo del vector de cada documento es 1), y aquellas palabras que tengan menor frecuencia de aparición serán más relevantes que las que aparezcan en más documentos."
      ],
      "metadata": {
        "id": "gQ0x_543Iewb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio\n",
        "\n",
        "Realiza la limpieza del dataset, la eliminación de stop-words, la vectorización del texto (bag of words) y la TF-IDF del siguiente *corpus* de documentos:\n",
        "\n",
        "> \"Cuando se juega al Juego de Tronos, solo se puede ganar o morir.\" - Cersei Lannister\n",
        "\n",
        "> \"Por qué será que en cuanto un hombre construye un muro, su vecino inmediatamente quiere saber qué hay del otro lado.\" - Tyrion Lannister\n",
        "\n",
        "> \"¿Qué es el honor, comparado con el amor de una mujer? ¿Qué es el deber, comparado con el calor de un hijo recién nacido entre los brazos, o el recuerdo de la sonrisa de un hermano? Aire y palabras. Aire y palabras. Solo somos humanos, y los dioses nos hicieron para el amor. Es nuestra mayor gloria y nuestra peor tragedia.\" - Maestre Aemon, Juego de Tronos\n",
        "\n",
        "> \"El hombre que dicta la condena debe blandir la espada.\" - Eddard Stark\n",
        "\n",
        "> \"El poder reside donde los hombres creen que reside. Es un truco, una sombra en la pared. Y un hombre muy pequeño puede proyectar una sombra muy grande.\" - Lord Varys"
      ],
      "metadata": {
        "id": "_rWIR9XACDvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"Cuando se juega al Juego de Tronos, solo se puede ganar o morir.\",\n",
        "    \"Por qué será que en cuanto un hombre construye un muro, su vecino inmediatamente quiere saber qué hay del otro lado.\",\n",
        "    \"¿Qué es el honor, comparado con el amor de una mujer? ¿Qué es el deber, comparado con el calor de un hijo recién nacido entre los brazos, o el recuerdo de la sonrisa de un hermano? Aire y palabras. Aire y palabras. Solo somos humanos, y los dioses nos hicieron para el amor. Es nuestra mayor gloria y nuestra peor tragedia.\",\n",
        "    \"El hombre que dicta la condena debe blandir la espada.\",\n",
        "    \"El poder reside donde los hombres creen que reside. Es un truco, una sombra en la pared. Y un hombre muy pequeño puede proyectar una sombra muy grande.\"\n",
        "]"
      ],
      "metadata": {
        "id": "uqug2lmKCDvO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Asegúrate de tener las stopwords descargadas\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Definir el nuevo corpus de texto\n",
        "corpus = [\n",
        "    \"Cuando se juega al Juego de Tronos, solo se puede ganar o morir.\",\n",
        "    \"Por qué será que en cuanto un hombre construye un muro, su vecino inmediatamente quiere saber qué hay del otro lado.\",\n",
        "    \"¿Qué es el honor, comparado con el amor de una mujer? ¿Qué es el deber, comparado con el calor de un hijo recién nacido entre los brazos, o el recuerdo de la sonrisa de un hermano? Aire y palabras. Aire y palabras. Solo somos humanos, y los dioses nos hicieron para el amor. Es nuestra mayor gloria y nuestra peor tragedia.\",\n",
        "    \"El hombre que dicta la condena debe blandir la espada.\",\n",
        "    \"El poder reside donde los hombres creen que reside. Es un truco, una sombra en la pared. Y un hombre muy pequeño puede proyectar una sombra muy grande.\"\n",
        "]\n",
        "\n",
        "# Eliminar la puntuación y caracteres no deseados\n",
        "chars = string.punctuation + '“”¡¿'\n",
        "re_punc = re.compile('[%s]' % re.escape(chars))\n",
        "corpus = [re_punc.sub('', texto) for texto in corpus]\n",
        "\n",
        "# Eliminar acentos\n",
        "def eliminar_acentos(texto):\n",
        "    texto_normalizado = unicodedata.normalize('NFD', texto)\n",
        "    texto_sin_acentos = ''.join(c for c in texto_normalizado if unicodedata.category(c) != 'Mn')\n",
        "    return texto_sin_acentos\n",
        "\n",
        "corpus = [eliminar_acentos(texto) for texto in corpus]\n",
        "\n",
        "# Convertir a minúsculas\n",
        "corpus = [texto.lower() for texto in corpus]\n",
        "\n",
        "# Eliminar stop-words\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "corpus_sin_stopwords = [' '.join([palabra for palabra in texto.split() if palabra not in stop_words]) for texto in corpus]\n",
        "\n",
        "# Vectorización (Bag of Words) usando CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_counts = count_vectorizer.fit_transform(corpus_sin_stopwords)\n",
        "\n",
        "# Transformación tf-idf\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Imprimir el resultado de la transformación tf-idf\n",
        "print(\"\\nVectorización tf-idf:\")\n",
        "print(X_tfidf.toarray())\n",
        "\n",
        "print(\"\\nCaracterísticas (palabras):\")\n",
        "print(count_vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "3LkunKfwLWsb",
        "outputId": "e78fc478-b242-4ee3-eec8-6108af158a30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vectorización tf-idf:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.39835162 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.39835162 0.39835162 0.         0.\n",
            "  0.39835162 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.32138758 0.\n",
            "  0.         0.         0.         0.         0.         0.32138758\n",
            "  0.         0.         0.         0.39835162 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.32532561 0.         0.32532561 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.21787435 0.         0.\n",
            "  0.         0.32532561 0.         0.         0.32532561 0.\n",
            "  0.         0.         0.32532561 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32532561\n",
            "  0.         0.         0.         0.32532561 0.32532561 0.\n",
            "  0.         0.         0.         0.         0.         0.32532561]\n",
            " [0.3397603  0.3397603  0.         0.16988015 0.16988015 0.3397603\n",
            "  0.         0.         0.         0.         0.         0.16988015\n",
            "  0.         0.16988015 0.         0.         0.16988015 0.\n",
            "  0.16988015 0.16988015 0.16988015 0.         0.         0.16988015\n",
            "  0.16988015 0.         0.         0.         0.         0.16988015\n",
            "  0.         0.16988015 0.         0.16988015 0.3397603  0.\n",
            "  0.16988015 0.         0.         0.         0.         0.\n",
            "  0.16988015 0.16988015 0.         0.         0.         0.13705824\n",
            "  0.         0.16988015 0.16988015 0.         0.         0.        ]\n",
            " [0.         0.         0.42841136 0.         0.         0.\n",
            "  0.42841136 0.         0.         0.         0.42841136 0.\n",
            "  0.42841136 0.         0.42841136 0.         0.         0.\n",
            "  0.         0.         0.         0.28691208 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.24182945 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.24182945\n",
            "  0.         0.         0.         0.161956   0.24182945 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.24182945\n",
            "  0.         0.24182945 0.24182945 0.24182945 0.19510648 0.\n",
            "  0.         0.         0.4836589  0.         0.         0.\n",
            "  0.4836589  0.         0.         0.         0.24182945 0.        ]]\n",
            "\n",
            "Características (palabras):\n",
            "['aire' 'amor' 'blandir' 'brazos' 'calor' 'comparado' 'condena'\n",
            " 'construye' 'creen' 'cuanto' 'debe' 'deber' 'dicta' 'dioses' 'espada'\n",
            " 'ganar' 'gloria' 'grande' 'hermano' 'hicieron' 'hijo' 'hombre' 'hombres'\n",
            " 'honor' 'humanos' 'inmediatamente' 'juega' 'juego' 'lado' 'mayor' 'morir'\n",
            " 'mujer' 'muro' 'nacido' 'palabras' 'pared' 'peor' 'pequeno' 'poder'\n",
            " 'proyectar' 'puede' 'quiere' 'recien' 'recuerdo' 'reside' 'saber' 'sera'\n",
            " 'solo' 'sombra' 'sonrisa' 'tragedia' 'tronos' 'truco' 'vecino']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}